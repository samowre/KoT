\documentclass[a4paper,12pt]{article}
\usepackage[utf8x]{inputenc}
\usepackage{setspace}

%opening
\title{Kernel of Truth based SAT proof checker}
\author{Andrei Dan}

\begin{document}
\noindent 
ECOLE POLYTECHNIQUE \\ 
PROMOTION 2007 \\ 
DAN Andrei Marian \\

\vspace{1in}

\begin{center}

 {\large RAPPORT DE STAGE DE RECHERCHE \\
\vspace{0.3in}
 {\LARGE Kernel of Truth based SAT proof checker} \\
\vspace{0.3in}
 NON CONFIDENTIEL}

\end{center}

\vspace{1in}
\noindent
Option: INFORMATIQUE
\\ Champ de l'option: Qualite du logiciel
\\ Directeur de l'option: Olivier Bournez
\\ Directeur de stage: Natarajan Shankar
\\ Dated du stage: 05/04/2010 - 27/08/2010
\\ Adresse de l'organisme: 
\\ Computer Science Laboratory
\\ SRI International
\\ 333 Ravenswood Avenue
\\ Menlo Park, CA 94025
\\ USA


\newpage
\verb| |
\vspace{2.2in}
\begin{abstract}
SAT solvers can generate proofs that justify their final result.
This report presents a verified program that checks if the generated proofs are valid. The checker is written and
proved in PVS. In order to execute it, Lisp code is automatically generated. 
We describe a trusted kernel called the Kernel of Truth. 
Finally, we prove that it exists a valid kernel proof corresponding to any valid checker proof. 
This demonstrates that the checker is a safe extension of the kernel.
\end{abstract}

\newpage
\verb| |
\vspace{0.8in}
\tableofcontents

\newpage

\section{Introduction}
Before starting, I would like to thank Dr. Natarajan Shankar for giving me the chance to work on this project and for helping me
throughout all my internship. I also had great help from Sam Owre and from Dr. Bruno Dutertre. \\
The correctness of SAT solver programs can be achieved by several approaches. One possibility is to specify the solver using
a proof asistant and to prove its correctness. In this scenario, the software that needs to be trusted contains the 
proof assistant and the compiler from specification language to programming language. The approach presented here 
is based on a trusted kernel called Kernel of Truth (KoT). KoT contains first-order deduction rules that can be composed to 
create more complex proofs. The proof generated by the solver has an equivalent kernel proof in KoT logic. The 
advantage of this approach is that we must only trust the KoT rules and the program that checks a KoT proof, instead of the entire
proof assistant.

Modern implementations of DPLL using clause learning are difficult to formalise and verify 
using proof assistants. This imposes the compromise between formalisation and performance. To avoid this choice, S
 accepts his uncertain status and generates proofs (noted Pr) that justifies his answers. 
The Pr for a formula can have the following forms:

\begin{enumerate}
\item An assignment for all the variables that makes the formula is satisfiable 
\item A list of clauses and resolution steps that resolve to the empty clause (other forms?)
\end{enumerate}

\section{Overview of PVS}
PVS  (Prototype Verification System) is an environment for specification and proving. The main purpose of PVS is to provide 
formal support for conceptualization and debugging in the early stages of the lifecycle of the design
of hardware or software systems. In these stages, both the requirements and designs are expressed in abstract
terms that are not necessarily executable. The best way to analyze such an abstract specification is by 
attempting proofs of desirable consequences of the specification. Subtle errors revealed by trying to prove
the properties are costly to detect and correct at later stages of the design lifecycle.
\\The specification language of PVS is built on higher-order logic (functions can be treated like
primitive types: functions can take functions as arguments and return them as values, quantification
can be applied to function variables. Specifications can be constructed using definitions and axioms \cite{prover}.


\subsection{The first proof}
Types declaration is done by declaring type constructors with associated parameters:
\begin{verbatim}
 list [T: TYPE]: DATATYPE 
 BEGIN
  null: null?
  cons (car: T, cdr:list):cons?
 END list
\end{verbatim}
Functions require the type specification for all the parameters and for the return value. Recursive 
functions need a measure that decreases with each call. For instance:
\begin{verbatim}
   member(x, l): RECURSIVE bool =
    CASES l OF
      null: FALSE,
      cons(hd, tl): x = hd OR member(x, tl)
    ENDCASES
   MEASURE length(l)
\end{verbatim}
Properties of the function's behavior can be proven using theorems or lemmas:
\begin{verbatim}
 member_null: LEMMA member(x, l) IMPLIES NOT null?(l)
\end{verbatim}
This lemma states that if \verb|x| is a member of the list \verb|l|, then the list is not empty.
When we try to prove this lemma, the proving environment displays a sequent. For this case, the 
sequent is the body of the lemma, and universal quantifiers are added for the variables \verb|x, l|.
\begin{verbatim}
  |-------
{1}   FORALL (l: list[T], x: T): 
	member(x, l) IMPLIES NOT null?(l)
\end{verbatim}
The lines above \verb|-------| are the known elements, and below are the objectives. It is 
considered that the known elements are in conjunction and the objectives are in disjunction. In 
this case there are no hypothesis.
We explain step by step the proof because proving properties was an important part of the work.
We apply proof commands that modify the objective using the hypothesis. In the example we use
\verb|(skosimp)| that does skolemization, eliminating the universal quantifiers, and simplifies
the resulting formula. We obtain:
\begin{verbatim}
{-1}  member(x!1, l!1)
{-2}  null?(l!1)
  |-------
\end{verbatim}
This means that there are now two hypothesis and no conclusion, meaning that there is a conflict
between the two formulas above the line. The skolemization transformed the variables \verb|x, l| in \verb|x!1, l!1|.
The simplification that followed the skolemization 
eliminated \verb|IMPLIES| by moving \verb|member(x!1,l!1)| above the line. Also, the \verb|NOT null?(l!1)|
formula below the line was transformed in \verb|null?(l!1)| above the line.
\\To continue the proof, we expand the definition of the member function by using the command \verb|(expand member)|. 
We obtain:
\begin{verbatim}
 {-1}  CASES l!1 OF 
	  null: FALSE, 
	  cons(hd, tl): x!1 = hd OR member(x!1, tl)
        ENDCASES
[-2]  null?(l!1)
  |-------
\end{verbatim}
The number of the formula can be in \verb|{}|, which means that it was modified by the previous command, or in \verb|[]|.
This corresponds to the function's definition above. We use \verb|(lift-if)| to replace the \verb|CASES| form with an 
\verb|IF|.
\begin{verbatim}
 {-1}  IF null?(l!1) THEN FALSE
      ELSE x!1 = car(l!1) OR member(x!1, cdr(l!1))
      ENDIF
[-2]  null?(l!1)
  |-------
\end{verbatim}
In this situation we can use the hypothesis \verb|[-2]| to eliminate the \verb|IF| from \verb|{-1}| by \verb|(replace -2 -1)|.
The proof simplifies to:
\begin{verbatim}
[-1]  null?(l!1)
  |-------
{1}   TRUE
\end{verbatim}
\verb|FALSE| in the hypothesis becomes \verb|TRUE| in the conclusion. The prover automatically gives the following message:
\begin{verbatim}
 which is trivially true.
Q.E.D.
\end{verbatim}


\subsection{TCCs}
The PVS typechecker analyzes the specification for semantic consistency. The type system of PVS is not 
algorithmically decidable. Theorem proving by the user might be
required to establish the type-consistency of a PVS specification. These theorems are called type-correctness conditions 
(TCCs) \cite{userguide}. For instance, the following function:
\begin{verbatim}
   append(l1, l2): RECURSIVE list[T] =
    CASES l1 OF
      null: l2,
      cons(x, y): cons(x, append(y, l2))
    ENDCASES
    MEASURE length(l1)
\end{verbatim}
generates the termination TCC:
\begin{verbatim}
 append_TCC1: OBLIGATION
  FORALL (l1: list[T], x: T, y: list[T]):
    l1 = cons(x, y) IMPLIES length(y) < length(l1);
\end{verbatim}
This TCC is created because the \verb|append| function declares that the length of the first argument should decrease 
in the recursive call. 

\subsection{Code generation}
PVS, like other specification languages, is designed to be expressive rather than executable. However, in order to test and
validate models, executing specifications is useful. This is achieved by generating Lisp code from PVS. The fragment of PVS
that can be executed can be viewed as a high-order functional programming language. Examples of specification elements that 
are not executable are: free variables, quantifications over infinite domains, terms with uninterpreted function symbols, 
equalities between terms of higher type \cite{eexec}.

\section{Presenting the Kernel of Truth}

\begin{quotation}
 The best way to find out if you can trust somebody is to trust them. (Ernest Hemingway)
\end{quotation}

This text presents the implementation of the concepts presented in \cite{rewip}.
The KoT is written in PVS.

\subsection{Formulas}

A term is defined reccursively as a variable or an application of a function to a list of terms. The 
variables are identified by an index. The functions are defined by two natural numbers, an index and the 
arity. The length of the list of arguments should be equal to the function's arity.

\begin{verbatim}
  term  : DATATYPE 
   BEGIN
   v(v_index: nat): var?
   apply(fun: (fun?),
         args: {ss: list[term] | 
		  length(ss) = arity(fun)}): apply?
  END term
\end{verbatim} 
The distinction is made between interpreted or uninterpreted predicates and functions.
\begin{verbatim}
 funpred: DATATYPE WITH SUBTYPES pred?, fun?
  BEGIN
   ipred(index, arity: nat): ipred? : pred?
   upred(index, arity: nat): upred? : pred?
   ifun(index, arity: nat): ifun?   : fun?
   ufun(index, arity: nat): ufun?   : fun?
  END funpred
\end{verbatim}

A formula is either a predicate applied to its arguments, a negation, a disjunction or an existential
quantification. The universal quantifier and the conjunction can be composed using the previous functions.

\begin{verbatim}
fmla: DATATYPE
  BEGIN
   atom(pred: (pred?), args: {ss: list[term] | 
			      length(ss) = arity(pred)}): atom?
   f_not(arg: fmla): f_not?
   f_or(arg1, arg2: fmla): f_or?
   f_exists(bvar: (var?), body: fmla): f_exists? 
  END fmla
\end{verbatim} 

The substitution of a variable in a term with another term, term equality, parameter substitution, 
free variables search are also implemented in the kernel. 
A sentence is a formula that doesn't have any free variables (all variables are bound by the existential 
quantificator). 

\begin{verbatim}
 A, B, C: VAR fmla
 sentence?(A): bool = null?(freevars(A))
\end{verbatim}
KoT uses a list of sentences to represent one-sided sequents, which are used to build the rules. \\
The equality between two terms uses a predefined predicate:
\begin{verbatim}
 f_eq(s, t): fmla = atom(ipred(0, 2), (:s, t :))
\end{verbatim}
The equality predicate has the index 0, the argument 2 representing the number of parameters.

\subsection{Rules}
\begin{itemize}
 \item The first induction rule defined in KoT is the axiom:
\begin{equation}
 \frac{}{\vdash A, \neg A, \ldots}
\end{equation} 

\item Subset rule:
\begin{equation}
 \frac{\vdash A_{1}, \ldots , A_{n}}{\vdash B_{1}, \ldots B_{m}} , {A_{1}, \ldots , A_{n}} \subset {B_{1}, \ldots B_{m}}
\end{equation}

\item Or rule:
\begin{equation}
 \frac{\vdash A, B}{\vdash A \bigvee B} 
\end{equation}  

\item Not-or rule:
\begin{equation}
 \frac{\vdash \neg A \; \; \; \; \vdash \neg B}{\vdash \neg (A \bigvee B)} 
\end{equation}  
 
\item Negation rule:
\begin{equation}
 \frac{\vdash A}{\vdash \neg \neg A} 
\end{equation}  

\item Cut rule:
\begin{equation}
 \frac{\vdash A, A_{1}, \ldots , A_{n} \; \; \; \; \vdash \neg A, B_{1}, \ldots B_{m}}
      {\vdash A_{1}, \ldots , A_{n}, B_{1}, \ldots B_{m}}
\end{equation}

\item Exists rule: 
 \begin{equation}
 \frac{\vdash [s/x]A} {\vdash \exists x:A} \; \; \; \;, freevars(s) = \emptyset
\end{equation}  

\item Equivalent forall rule:
 \begin{equation}
 \frac{\vdash \neg [f/x]A} {\vdash \neg \exists x:A} \; \; \; \;, \neg f \in freesymbols(A) 
\end{equation}  

\item Axiom schema instantiation rule, appliable for function and predicate instantiation.

\item The reflexivity rule, where the equality is the predicate described above:
\begin{equation}
\frac{}{\vdash s=s} 
\end{equation}

\item Function congruence rule:
\begin{equation}
 \frac{}{\vdash \neg (a1 = b1), \ldots, f(a1, \ldots) = f(b1,\ldots)}
\end{equation}

\item Predicate congruence rule:
\begin{equation}
 \frac{}{\vdash \neg (a1 = b1), \ldots, \neg p(b1, \ldots), p(a1, \ldots) }
\end{equation}

\end{itemize}

\section{The trace checker}
SAT solvers take as an input a logic formula and return a boolean value. To justify their output, SAT solvers can also return a
proof. If the formula is satisfiable, the proof is an assignment for each variable that make the formula true. If the formula 
is not satisfiable, then the solver provides a list of resolution steps that starts with the initial clauses and leads to 
the empty clause \cite{zhang}. This is the proof that the initial clauses lead to a conflict, so a satisfying assignment does not exist.
The trace checker presented in this text verifies that the list of resolution steps actually leads to the empty clause. 
The checker can generate a tree proof containing only KoT rules for each list of resolution steps justifying the resulted clause.

\subsection{Input data}
The Picosat solver generates a trace (tP) to justify the result. The line input format for the trace checker (tC) is similar 
to tP:
\begin{enumerate}
 \item 
\begin{equation}
 <lineNumber><literalsList>s \;\; s, 
\end{equation} where $lineNumber \ge 0$, $ literalsList $ is a list of integers representing the normal or negated form of the variables in the 
initial CNF formula. 
$ s $ has the role of separator.

For instance: 
\begin{verbatim}18 -40 -55 90 s s\end{verbatim}
   is the 19th line of the file and represents the clause \begin{verbatim}(-40 -55 90)\end{verbatim}
\item
\begin{equation}
 <lineNumber>*<lineNumbers>s,
\end{equation}
where $ lineNumbers $ is a list of naturals representing the lines of the clauses on which resolution must be applied. 
Each element of $ literalsList$ is smaller than $ lineNumber $, meaning that only previous clauses can be used. An example is 
\begin{verbatim}
  57 * 32 29 46 45 56 s,
\end{verbatim} a line that requires resolution to be applied to the clauses obtained at the lines 
\begin{verbatim}32, 29, 46, 45, 56\end{verbatim}

\end{enumerate}

The differences between tC and tP are: 
\begin{enumerate}
 \item In tP the separator is 0, and the line numbers begin at 1.
 \item In tP the line numbers can have gaps, consecutive lines can be numbered $ n $ and $n + 1+\delta$, where $\delta > 0$. In tC
 the gaps are eliminated, modifying the $lineNumber$, and its appearances in the $lineNumbers$ lists.
 \item tP presents a general type of line, that contains both $literalsList$ and $lineNumbers$. In this case, the clause that
should be obtained by resolution on the list of clauses is given. The role of the trace checker would be only to verify that this
clause and the computed clause are the same. This type of line was not generated by Picosat during the tests 
and it is not treated by tC.
 \item In tP, the order of $lineNumbers$ is not necesarly the correct one to obtain at the end the empty clause. In tC, the order
in which the resolutions are applied is the order indicated by $lineNumbers$.
\end{enumerate}
The transformation from tP to tC is done by a Lisp program and uses a slightly modified Picosat solver that generates the 
$lineNumbers$ in the correct order.
\\The PVSio library \cite{pvsio} was used as an interface between file-input operations and the PVS structures.

\subsection{Checker structure}

The literals of the logic expression are KoT formulas: uninterpreted predicates of arity 0. \verb|prop_atom?| represents a variable, 
and literals are variables or negated variables:
\begin{verbatim}
 prop_atom?(A): bool = (atom?(A) AND 
    	      	           upred?(pred(A)) AND 
			   arity(pred(A)) = 0 AND
			   index(pred(A)) > 0)

 literal?(A): bool = (prop_atom?(A) OR 
    		         (f_not?(A) AND prop_atom?(arg(A))))
\end{verbatim}
The clause type is defined by a predicate that describes a strict sorted list of literals. The lists are sorted to accelerate the resolution step 
and to avoid duplicate literals in a clause.
\begin{verbatim}
 lAA : VAR list[(literal?)]
 clause?(lAA) : bool = sorted?(lAA)
\end{verbatim}

The function \verb|nclause2fmla| takes a non-empty clause and returns an equivalent formula built as a disjunction
of the literals in the clause.  
\begin{verbatim}
     nclause2fmla(nclAA): RECURSIVE fmla= 
      IF null?(cdr(nclAA)) 
      THEN car(nclAA)
      ELSE f_or(car(nclAA), nclause2fmla(cdr(nclAA)))
      ENDIF
    MEASURE length(nclAA)
\end{verbatim}

The function \verb|not_or_reduction| transforms a non-empty clause into a sequent(a list of formulas with no 
free variables) containing the negated formula returned by \verb|nclause2fmla|:
\begin{verbatim}
 not_or_reduction(nclAA): sequent = 
      cons(f_not(nclause2fmla(nclAA)), null) 
\end{verbatim}
The previous function will be used in the equivalence theorems presented in the next section.
The resolution is based on a pivot, a variable that is found in a clause and its negation in the other clause. 
The pivot is not specified in the trace that is being verified, so the checker must find it. The function
that verifies if a pivot exists takes as arguments the two clauses:
\begin{verbatim}
 exist_pivot?(ck, cl) : RECURSIVE bool =
    CASES ck OF 
      null: FALSE,
      cons(k, cm): member(tr_neg(k), cl) 
		    OR exist_pivot?(cm, cl)
    ENDCASES
  MEASURE length(ck)
\end{verbatim}
The signature of the function that returns the pivot, once it is established that it exists is:
\begin{verbatim}
 find_pivot(nck, (ncl:{ncm | exist_pivot?(nck, ncm)})) : 
   RECURSIVE {k | member(k, nck) AND member(tr_neg(k), ncl)}
\end{verbatim}
where \verb|ck, cl| are clauses and \verb|nck, ncl, ncm| are non-empty clauses.
Using the sorted list representation for the clauses, the resolution step becomes a merge of the two clauses, 
followed by removing the pivot. The checker allows the situation where a pivot doesn't exist. In this case, the 
resolution result is only the merged list. The function \verb|tr_clause_true?| returns true if a clause contains
a literal and its negation. The resolution function is:
\begin{verbatim}
 resolution(nck, ncl) : (tr_clause?) = 
    IF tr_clause_true?(nck) THEN ncl
    ELSIF tr_clause_true?(ncl) THEN nck
    ELSE
     LET merged = merge(nck, ncl) IN
     IF exist_pivot?(nck, ncl) THEN
       LET pivot= find_pivot(nck, ncl) IN 
       delete_pivot(merged, pivot) 
     ELSE
       merged
     ENDIF
    ENDIF
\end{verbatim}
The \verb|resolution_list| function is equivalent to a fold applied on the list of clauses with the 
\verb|resolution| function.
\\For each line of the certificate, in succesive order, there is either no change 
(the case of input clauses) or \verb|resolution_list| is applied, in order to calculate the line's clause.
A certificate is correct is at the final line the empty clause is obtained.

\subsection{Kernel of Truth equivalent proof}
The type \verb|rule| corresponds to the induction rules presented above. Each rule has a constructor associated. 
The proofs are stored as a finite sequence of proof steps:
\begin{verbatim}
 proof_step : TYPE
   = [# sequent: sequent,
        rule: rule,
	subs: list[nat] #]

 proof_seq: TYPE =  finseq[proof_step]
\end{verbatim}

The \verb|sequent| is the conclusion of the \verb|proof step|. The \verb|rule| is used to prove the \verb|sequent|.
\verb|subs| is a list of indices, each element of the list pointing to an element in the proof that is used as
a premise for the \verb|rule| that is applied.
\\The conclusion of a proof is the \verb|sequent| of the last proof step in the list:
\begin{verbatim}
 RR: VAR proof_seq
 ne_proof_seq: TYPE = {RR | RR`length > 0}
 nRR: VAR ne_proof_seq

 conclusion(nRR): sequent =
    (nRR`seq(nRR`length - 1))`sequent
\end{verbatim}

An important function of KoT is \verb|checkProof| which takes as input a finite sequence of lemmas that can be used
and the proof. The function checks the correctness of each element of the proof. The signature of this function is:
\begin{verbatim}
 checkProof(lemmas: finseq[sequent])(RR): bool
\end{verbatim}
The objective is to build proofs for a resolution step and then for a chain of such steps that are validated by \verb|checkProof|.
\\Auxiliary functions are used to build proofs. Let's describe the function that composes the proof of 
$A:\vdash(\neg a), \Delta$ with the proof of $B:\vdash(\neg b), \Delta$ to obtain a proof of 
$C:\vdash \neg (a \vee b), \Delta$. If the proof of $A$ is a sequence of $n$ proof steps and the proof of $B$
has length $m$, then the proof of $C$ will have the length $n+m+1$. The first $n+m$ steps are the concatenation
of the first two proofs, and the last step is a $not-or$ rule. The function that builds the last proof step is:
\begin{verbatim}
 concl_norr(sA, sB, sAA, (n, m: posnat)): proof_step = 
          (# sequent:= cons(f_not(f_or(sA, sB)), sAA) , 
    	     rule:= norr,
	     subs:= cons(n - 1, cons(n + m - 1,null)) #)
\end{verbatim}
\verb|sA| and \verb|sB| are $a$ and $b$ from the formulas above, \verb|sAA| is $\Delta$, and \verb|m,n| are the 
lengths of the proofs. \verb|subs| is 
\\The proof is built in PVS by:
\begin{verbatim}
 RR_norr(sA, sB, sAA, RR1, RR2): ne_proof_seq = 
    LET n = RR1`length IN
    LET m = RR2`length IN
    (# length := n + m + 1,
       seq := LAMBDA(j: below(n + m + 1)):
       	      	 COND
		  j < n -> RR1`seq(j),
		  j >=n AND j < n + m -> offset_proof_step(RR2`seq(j - n), n), 
		  j = n + m -> concl_norr(sA, sB, sAA, n, m)
		 ENDCOND #)   
\end{verbatim}
\verb|RR1, RR2| are the two finite sequences representing the proofs of $A$ and $B$. The resulting sequence
is a function $[0 \ldots n+m] \rightarrow$ \verb|proof_step|, identical with \verb|RR1| on $[0 \ldots n-1]$, with 
\verb|RR2| on $[n \ldots n+m-1]$, and in $n+m$ the value is the result of \verb|concl_norr|.
\\The PVS lemma resuming all the description above is:
\begin{verbatim}
 lemma_norr: LEMMA (conclusion(RR1) = cons(f_not(sA), sAA) AND
		       conclusion(RR2) = cons(f_not(sB), sAA) AND
		       checkProof(empty_seq)(RR1) AND
		       checkProof(empty_seq)(RR2))
		      =>
		      (conclusion(RR_norr(sA, sB, sAA, RR1, RR2)) = 
			cons(f_not(f_or(sA, sB)), sAA) AND
		       checkProof(empty_seq)(RR_norr(sA, sB, sAA, RR1, RR2)))
\end{verbatim}
This shows that if the proofs \verb|RR1, RR2| are validated by \verb|checkProof|, then the proof returned by 
\verb|RR_norr| will also be validated and it will have the required conclusion.
\subsubsection{Resolution proof}
First, a theorem about single step resolution is proved:
\begin{verbatim}
 th: THEOREM  
      	conclusion(proof_th(ntcA, ntcB)) = 
	 append(
	  not_or_reduction(translate_clause(ntcA)), 
	  append(
	  	   not_or_reduction(translate_clause(ntcB)), 
	    translate_clause(resolution(ntcA, ntcB))))
	AND checkProof(empty_seq)(proof_th(ntcA, ntcB))
\end{verbatim}

The function \verb|proof_th| builds a proof that has the following structure:
\begin{equation}
 \frac{\displaystyle 
  \frac{\displaystyle 
   \frac{\displaystyle 
    \frac{axiom}{\vdash \neg p, \neg \neg p} \;\;\;\;
    \frac{axiom}{\vdash \neg \Delta, \Delta}}{\vdash \neg p, \neg (\neg p \bigvee \Delta), \Delta} \;\;\;\; 
   \frac{\displaystyle axiom}{\vdash \neg \Gamma, \Gamma}}{\vdash \neg(p \bigvee \Gamma), \neg(\neg p \bigvee \Delta), \Gamma, \Delta}}
  {\vdash \neg(p \bigvee \Gamma), \neg(\neg p \bigvee \Delta), \Gamma \bigvee \Delta}
\end{equation}
 \subsubsection{Chain resolution proof}
We use several auxiliary functions that operate on sequents. \verb|not_or_map| takes a non-empty clause as an argument and 
returns the sequent containing the negated literals of the clause.
\\The variable \verb|lntcA| is a non-empty list of clauses.
Since the checker must resolve all the clauses in a chain to obtain the resolvent, we build a theorem to prove the KoT equivalence
for chain resolution. Similar to the resolution step, we prove:
\begin{verbatim}
   th_list:  THEOREM
  	LET result: (tr_clause?) = 
	  resolution_list(lntcA) IN
        conclusion(proof_th_list(lntcA)) = 
	 append(
	   not_or_map(lntcA), 
	   translate_clause(result))
	AND checkProof(empty_seq)(proof_th_list(lntcA))
\end{verbatim}
The variable \verb|result| contains the resolvent obtained by resolution on the list of clauses. The function 
\verb|proof_th_list| builds the proof that concludes that the chain resolution on a list of clauses is implied by the clauses.

\section{Examples and tests}
Picosat is the SAT solver used for proof generation. A modified version of Booleforce takes a proof generated by Picosat and
reorders the antecedents of the learned clauses such that chain resolution can be applied. 
\subsection{A small example}
We start with a simple unsatisfiable CNF formula:
\begin{verbatim}
p cnf 2 4
1 2 0
-1 2 0
1 -2 0
-1 -2 0
\end{verbatim}
This file corresponds to $(x \bigvee y) \bigwedge (\neg x \bigvee y) \bigwedge (x \bigvee \neg y) \bigwedge (\neg x \bigvee \neg y)$
The solver calculates that the formula is unsat and produces a trace to prove this result. After the processing, the proof 
becomes:
\begin{verbatim}
0 1 2 s s
1 -1 2 s s
2 1 -2 s s
3 -1 -2 s s
4 * 1 3 s
5 * 4 2 s
6 * 4 0 5 s
\end{verbatim}
This can be easily checked by hand. The first number of the line represents the clause identifier. The first four lines are 
the initial clauses. The clause associated with the fifth line is obtained by resolving clauses \verb|1| and \verb|3|, thus obtaining \verb|(-1)|.
The sixth clause is \verb |-2|. For the last line, we resolve clause 4 with 0, and the result with clause 5. The result is the empty
clause, so this is a valid unsatisfiability proof.

\subsection{Complexity Analysis}
The checker has a linear complexity $O(n)$, where $n$ is the number of resolution steps specified by the trace. The complexity constant
that multiplies $n$ is the source of possible optimisations. Since the code generated from PVS comes from a specification, there is
a compromise between the efficiency of the code and the ease to prove the required properties. Since both PVS and Lisp 
are functional languages, an important optimisation is tail-recursive functions. We should make sure that tail-recursive functions
specified in PVS are translated in Lisp functions with the same property. Another method to improve the speed of the checker is to 
build a faster version of the resolution function and then prove only the equivalence with the already proved function. We used this 
method to accelerate the checking process.

\subsection{Tests}
We used CNF formulas from the SatLib benchmarks problems and other sources. The results are presented in the table below: 
\begin{figure}

\begin{tabular}{|c|c|}
\hline
 Size of the trace (KB) & Time to check (s) \\
\hline
1.2 & 0.003  \\
1.6 & 0.015  \\
76 & 4.7 \\
449 & 20 \\
963 & 74 \\ 
1800 & 132 \\
15300 & 1006\\
37800 & 1863 \\
\hline
\end{tabular} 

\end{figure}


The current version that produced these results is not optimised for efficient resolution steps, the accent being on the clear 
specification and ease of proving. A new version, without fundamental modifications, could reduce times because currently
the clauses are parsed several times in order do perform resolution, whereas a single parse could be enough.

\section{Conclusion}
In this report we present the structure of a trusted kernel (KoT) and the implementation of a cerfified SAT trace checker. We present
the proof that it exists a valid kernel proof corresponding to any valid checker proof. This project required writing a preprocessor for the traces
in Lisp, proving the checker in PVS and using a PVS extension, PVSio, to be able to write imperative code when needed. 
It is possible to continue the extension of the kernel by building verified checkers for SMT and rewrite traces. 

\bibliography{raport_full}
\bibliographystyle{plain}


\end{document}
